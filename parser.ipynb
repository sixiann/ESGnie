{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a88624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import floor, ceil\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "from nltk import corpus\n",
    "import string\n",
    "import itertools\n",
    "import boto3\n",
    "import re\n",
    "import io\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46677ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_S3_BUCKET = 'esgnie'\n",
    "\n",
    "def get_s3_companies(s3_client, bucket='esgnie', comp_prefix='companies/'):\n",
    "    ## get companies\n",
    "    result = s3_client.list_objects(Bucket=bucket, Prefix=comp_prefix, Delimiter='/')\n",
    "    l_subdir = [com_prefix.get('Prefix').replace(comp_prefix, '').replace('/', '')\n",
    "                for com_prefix in result.get('CommonPrefixes')]\n",
    "    return l_subdir\n",
    "\n",
    "def get_company_files(s3_client, ticker, bucket='esgnie', filename_pattern=None, comp_prefix='companies'):\n",
    "    result = s3_client.list_objects(Bucket=bucket, Prefix=f\"{comp_prefix}/{ticker}/\")\n",
    "    df_files = pd.DataFrame()\n",
    "    df_files['file'] = [obj['Key'] for obj in result['Contents']]\n",
    "    df_files['filename'] = [file.split('/')[-1] for file in df_files['file']]\n",
    "    if filename_pattern is not None:\n",
    "        df_files['pattern_match'] = [1 if re.match(filename_pattern, file) else 0 for file in df_files['filename']]\n",
    "    # df_files[df_files['pattern_match'] == 1]['filename'].tolist()\n",
    "    return df_files[df_files['pattern_match'] == 1]['file'].tolist()\n",
    "\n",
    "def write_csv_to_s3(s3_client, df, file, bucket='esgnie'):\n",
    "    with io.StringIO() as csv_buffer:\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "        response = s3_client.put_object(\n",
    "            Bucket=bucket, Key=file, Body=csv_buffer.getvalue()\n",
    "        )\n",
    "        status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "    return status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ae8669",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "l_ticker = get_s3_companies(s3_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a32325f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quants_parser(df):\n",
    "    '''\n",
    "    input: df_cell file \n",
    "    output: header_rows, non_quant_rows, quant_rows, quant_rows information \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = df.drop(['Unnamed: 0', \"file\", 'level_3', 'xbar','ybar', \n",
    "                  'upright', 'direction','digits_ratio', 'years_ratio',\n",
    "                  'dates_ratio', 'keyword_flag','keyword_col_flag','keyword_row_flag'], axis=1)\n",
    "    \n",
    "    df[\"text_height\"] = df.bottom-df.top\n",
    "\n",
    "    is_numeric = []\n",
    "    for text in list(df.text):  \n",
    "        t=\"\".join(c for c in text if c.isalnum())\n",
    "        is_numeric.append(t.isnumeric())\n",
    "\n",
    "    df[\"numeric_flag\"] = is_numeric\n",
    "    \n",
    "        ###masks\n",
    "    quantmask1 = df[\"quant_row_flag\"] == 1\n",
    "    yearmask1 = df[\"year_flag\"] == 1\n",
    "    \n",
    "    if len(df.text_height.mode()) == 1:\n",
    "        heightmask = df[\"text_height\"] > float(df.text_height.mode())\n",
    "    else: \n",
    "        heightmask = df[\"text_height\"] > float(df.text_height.min())\n",
    "        \n",
    "    textmask = df[\"numeric_flag\"] == 0\n",
    "    \n",
    "    header_rows = df[heightmask & ~quantmask1 & textmask]\n",
    "    items = df[quantmask1 & ~yearmask1 & ~heightmask & textmask]\n",
    "    nonquantrows = pd.concat([items, df]).drop_duplicates(keep=False)\n",
    "    \n",
    "    mask1 = items[\"text\"] != \"Indicator\"\n",
    "    mask2 = items[\"text\"] != \"Items\"\n",
    "\n",
    "    quant_rows = items[mask1 & mask2].row_num.unique()\n",
    "    quantmask2 = df[\"row_num\"].isin(quant_rows)\n",
    "\n",
    "    quant_rows = df[quantmask2 & ~textmask]\n",
    "\n",
    "    rows_of_df = []\n",
    "    columns_of_df = [\"Company\", \"File Name\", \"Page Number\", \"Variable\", \"Date\", \"Value\"]\n",
    "\n",
    "\n",
    "    def f(qr):\n",
    "        left_coord = qr.x0\n",
    "        right_coord = qr.x1\n",
    "        leftmask = df.x0  >= math.floor(qr.x0) -25 \n",
    "        rightmask = df.x1 <= math.ceil(qr.x1) + 25\n",
    "        rowmask2 = df.row_num <= qr.row_num\n",
    "        quantmask2 = df.row_num.isin(quant_rows)\n",
    "        \n",
    "        company = df.company.iloc[0]\n",
    "        filename = df.filename.iloc[0]\n",
    "        pagenum = df.pagenum.iloc[0]\n",
    "        \n",
    "        if items[items.row_num == qr.row_num].empty:\n",
    "            variable = \" \"\n",
    "        else:\n",
    "            variable = items[items.row_num == qr.row_num].iloc[0].text\n",
    "        \n",
    "        if df[~quantmask2 & leftmask & rightmask & rowmask2 & yearmask1].empty:\n",
    "            date = \" \"\n",
    "        else:\n",
    "            date = df[~quantmask2 & leftmask & rightmask & rowmask2 & yearmask1].iloc[0].text\n",
    "        \n",
    "        value = qr.text\n",
    "\n",
    "        row = [company, filename, pagenum, variable, date, value]\n",
    "        rows_of_df.append(row)\n",
    "        return rows_of_df\n",
    "\n",
    "    quant_rows.apply(f,axis = 1)\n",
    "\n",
    "    df2 = pd.DataFrame(rows_of_df, columns = columns_of_df)\n",
    "    #df2 = df2[df2[\"Date\"] != \" \"]\n",
    "    \n",
    "    return header_rows, nonquantrows, items, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7f0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textfinder(items):\n",
    "    L = list(items.text)\n",
    "\n",
    "    words = ' '.join(L).lower()\n",
    "    words = words.translate(words.maketrans('', '', string.digits))\n",
    "\n",
    "    L = words.split(\" \")\n",
    "    total_num_of_words = len(L)\n",
    "\n",
    "\n",
    "    dic = Counter(L)\n",
    "    sortedtext = {k: v for k, v in sorted(dic.items(), key=lambda item: item[1], reverse = True)}\n",
    "    proportion = np.array(list(sortedtext.values()))/total_num_of_words\n",
    "    \n",
    "    if items.company.empty:\n",
    "        ticker = []\n",
    "    else:\n",
    "        ticker = list(items.company)[0] * np.ones(len(proportion))\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame.from_dict({\"company\": ticker, \"word\": sortedtext.keys(),\n",
    "                                 \"freq\": sortedtext.values(), \"proportion\": proportion})\n",
    "    \n",
    "    return words, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a363c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def commonwords(final):\n",
    "    custom = [\"\", \"must\", \"co.\", \"ltd.\", \"()\", \"')\", \"co.,\", \"(cid:)\", \",,.\", \",,,.\", \"year\", \"rmb\", \"december\", \"total\", \"company\",\n",
    "         \"china\", \"group\", \"others\", \"two\", \"within\", \"one\", \"item\", \"million\", \"billion\", \"sinopec\", \"–\", \"—\", \",,\", \"(,)\"]\n",
    "    stopwords = corpus.stopwords.words('english')\n",
    "    \n",
    "    stopwords = stopwords + list(string.punctuation) + list(string.digits) + custom\n",
    "    words, df = textfinder(final)\n",
    "    sw = df.word.isin(stopwords)\n",
    "    \n",
    "    #wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", height = 300, max_words = 100).generate(words)\n",
    "    #plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    #plt.axis(\"off\")\n",
    "    #plt.show()\n",
    "    \n",
    "    return df[~sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c7bd8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_by_ticker(ticker, groupby_variables = None, groupby_values = None, word_freq = False):\n",
    "    '''\n",
    "    input: company ticker\n",
    "    output: everything\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    l_files = get_company_files(s3_client, ticker=ticker, filename_pattern='^df_cells')\n",
    "    \n",
    "    print(len(l_files))\n",
    "    finalDF = pd.DataFrame()\n",
    "    finalITEMS = pd.DataFrame()\n",
    "    finalNONQUANT = pd.DataFrame()\n",
    "    \n",
    "    for file in l_files:\n",
    "        obj = s3_client.get_object(Bucket='esgnie', Key=file)\n",
    "        df = pd.read_csv(obj['Body'])\n",
    "        \n",
    "        if groupby_variables != None:\n",
    "            for i in range(len(groupby_variables)):\n",
    "                df = df[df[groupby_variables[i]] == groupby_values[i]]\n",
    "            \n",
    "            \n",
    "        header_rows, nonquantrows, items, df2 = quants_parser(df)\n",
    "        finalDF = finalDF.append(df2)\n",
    "        finalITEMS = finalITEMS.append(items)\n",
    "        finalNONQUANT = finalNONQUANT.append(nonquantrows)\n",
    "    \n",
    "    if word_freq == True:\n",
    "    \n",
    "        items = commonwords(finalITEMS)\n",
    "        nonquant = commonwords(finalNONQUANT)\n",
    "        overall = items.rename(columns={\"freq\": \"freq_quant_row\", \"proportion\": \"proportion_quant_row\"})\n",
    "\n",
    "\n",
    "        freq_nonquant_row = []\n",
    "        proportion_nonquant_row = []\n",
    "\n",
    "        for word in items.word:\n",
    "            if word in list(nonquant.word):\n",
    "                freq_nonquant_row.append(int(nonquant[nonquant.word == word].freq))\n",
    "                proportion_nonquant_row.append(float(nonquant[nonquant.word == word].proportion))\n",
    "            else:\n",
    "                freq_nonquant_row.append(0)\n",
    "                proportion_nonquant_row.append(0)\n",
    "\n",
    "        overall[\"freq_nonquant_row\"] = freq_nonquant_row\n",
    "        overall[\"proportion_nonquant_row\"] = proportion_nonquant_row\n",
    "        overall[\"freq_diff\"] = np.array(overall.freq_quant_row) - np.array(freq_nonquant_row)\n",
    "        overall[\"proportion_diff\"] = np.array(overall.proportion_quant_row) - np.array(proportion_nonquant_row)\n",
    "\n",
    "        overall.sort_values(by=['proportion_diff'], ascending = False, inplace = True)\n",
    "        \n",
    "        status = write_csv_to_s3(s3_client, overall, \"word_freq/%s_overall.csv\"%ticker, bucket='esgnie')\n",
    "        \n",
    "        if status == 200:\n",
    "            print(\"Successful saving word frequencies for %s\"%(ticker))\n",
    "        else:\n",
    "            print(\"Unsuccessful saving word frequencies for %s\"%(ticker))\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        items, nonquant, overall = None, None, None\n",
    "        \n",
    "    \n",
    "    #save files\n",
    "    status = write_csv_to_s3(s3_client, finalDF,\"tables/%s.csv\" % ticker, bucket='esgnie')\n",
    "    if status == 200:\n",
    "        print(\"Successful saving tables for %s\"%(ticker))\n",
    "    else:\n",
    "        print(\"Unsuccessful saving tables for %s\"%(ticker))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    return finalDF, finalITEMS, finalNONQUANT, items, nonquant, overall\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "337e974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordfreq(l_ticker, groupby_variables = None, groupby_values = None):\n",
    "    '''\n",
    "    inputs:\n",
    "    l_ticker: list of companies for which to parse for word frequencies\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    overall_words = pd.DataFrame()\n",
    "    \n",
    "    for ticker in l_ticker:\n",
    "        finalDF, finalITEMS, finalNONQUANT, items, nonquant, overall = parse_by_ticker(ticker, \n",
    "                                                                                       groupby_variables = groupby_variables,\n",
    "                                                                                       groupby_values = groupby_values,\n",
    "                                                                                      word_freq = True)\n",
    "        overall_words.append(overall)\n",
    "        \n",
    "    status = write_csv_to_s3(s3_client, overall_words, \"word_freq/%s_overall.csv\"%l_ticker, bucket='esgnie')\n",
    "        \n",
    "    if status == 200:\n",
    "        print(\"Successful saving word frequencies for %s\"%(l_ticker))\n",
    "    else:\n",
    "        print(\"Unsuccessful saving word frequencies for %s\"%(l_ticker))\n",
    "\n",
    "  \n",
    "    return overall_words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a53eb",
   "metadata": {},
   "source": [
    "# Actual parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34219697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful saving word frequencies\n",
      "Successful saving tables\n"
     ]
    }
   ],
   "source": [
    "ticker = l_ticker[0]\n",
    "finalDF, finalITEMS, finalNONQUANT, items, nonquant, overall = parse_by_ticker(ticker, groupby_variables = None, groupby_values = None, word_freq = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9914bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful saving word frequencies\n",
      "Successful saving tables for 601288\n",
      "Successful saving word frequencies\n",
      "Successful saving tables for 601688\n",
      "Successful saving word frequencies\n",
      "Successful saving tables for 601766\n"
     ]
    }
   ],
   "source": [
    "for ticker in l_ticker[2:]:\n",
    "    try:\n",
    "        finalDF, finalITEMS, finalNONQUANT, items, nonquant, overall = parse_by_ticker(ticker, groupby_variables = None, groupby_values = None, word_freq = True)\n",
    "    except (RuntimeError, TypeError, NameError, ValueError, ZeroDivisionError):\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edbf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordfreq(l_ticker[:2], groupby_variables = [\"pagenum\"], groupby_values = [20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecf86cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful saving word frequencies for 601857\n",
      "Successful saving tables for 601857\n"
     ]
    }
   ],
   "source": [
    "finalDF, finalITEMS, finalNONQUANT, items, nonquant, overall = parse_by_ticker('601857', groupby_variables = None, groupby_values = None, word_freq = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15f37a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in l_ticker[-4:]:\n",
    "    try:\n",
    "        finalDF, finalITEMS, finalNONQUANT, items, nonquant, overall = parse_by_ticker(ticker, groupby_variables = None, groupby_values = None, word_freq = True)\n",
    "    except (RuntimeError, TypeError, NameError, ValueError, ZeroDivisionError, AttributeError):\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48875f24",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1594dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vale = pd.read_csv(r\"F:\\Downloads\\df_cells_2011-sustainability-report_pagenum-51.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "835c016e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VALE3'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_ticker[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48ab12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_files = get_company_files(s3_client, ticker=l_ticker[-4], filename_pattern='^df_cells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96d254a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7b1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
